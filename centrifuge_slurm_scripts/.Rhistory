memory = "li:nth-child(3) small",
hard_drive = "li:nth-child(4) small",
display = ".ups-feature .text-gray-dark"
)
webpage <- webpages_ls[[1]]
scrape <- function(element, webpage)  {
# element <- elements[6]
#Using CSS selectors to scrape the rankings section
data_html <- html_nodes(webpage, element)
#Converting the ranking data to text
data_all <- html_text(data_html)
## remove bad data
data <- data_all %>%
{.[(nchar(.) < 100 & nchar(.) > 0)]}
#Let's have a look at the rankings
as.data.frame(data)
}
temp <- lapply(elements, scrape, webpage)
View(temp)
temp[["memory"]]
scrape <- function(element, webpage)  {
# element <- elements[6]
#Using CSS selectors to scrape the rankings section
data_html <- html_nodes(webpage, element)
scrape <- function(element, webpage)  {
scrape <- function(element, webpage)  {
# element <- elements[6]
#Using CSS selectors to scrape the rankings section
data_html <- html_nodes(webpage, element)
#Converting the ranking data to text
data_all <- html_text(data_html)
## remove bad data
data <- data_all %>%
{.[(nchar(.) < 100 )]}#& nchar(.) > 0)]}
#Let's have a look at the rankings
as.data.frame(data)
}
temp <- lapply(elements, scrape, webpage)
View(temp)
temp[["os"]]
temp[["memory"]]
temp[["name"]]
test <- list(element1=list(a=1,b=2,a1=8),
element2=list(a=9,d=17))
test
vec <- c(12,25)
vec
# for (i in 1:length(test)){
#     test[[i]] <- c(test[[i]],vec[i])
# }
merge <- mapply(c, test, vec)
test <- list(element1=list(a=1,b=2,a1=8),
element2=list(a=9,d=17))
test
vec <- c(12,25)
vec
# for (i in 1:length(test)){
#     test[[i]] <- c(test[[i]],vec[i])
# }
merge <- mapply(c, test, vec)
merge
merge <- mapply(c, vec, test)
merge
test <- list(element1=list(a=1,b=2,a1=8),
element2=list(a=9,d=17))
test
vec <- c(12,25)
vec
# for (i in 1:length(test)){
#     test[[i]] <- c(test[[i]],vec[i])
# }
merge <- mapply(c, vec, test)
merge
merge <- mapply(c, test, vec)
merge
temp <- webpages_ls %>%
mapply(c, ., urls)
View(temp)
View(webpages_ls)
View(webpages_ls)
temp <- webpages_ls %>%
mapply(function(x,y) x[["url"]] <- y, ,, urls)
temp <- webpages_ls %>%
{mapply(function(x,y) x[["url"]] <- y, ,, urls)}
temp <- webpages_ls %>%
{mapply(function(x,y) x[["url"]] <- y, ,. urls)}
temp <- webpages_ls %>%
{mapply(function(x,y) x[["url"]] <- y, ., urls)}
temp
temp <- webpages_ls %>%
# {mapply(function(x,y) x[["url"]] <- y, ., urls)}
{mapply(function(x,y) x[["url"]] <- y, ., urls)}
temp <- webpages_ls %>%
# {mapply(function(x,y) x[["url"]] <- y, ., urls)}
{mapply(
function(x,y) {
print(x),
cat(y)
}
.,
urls
)}
temp <- webpages_ls %>%
{mapply(
function(x,y) {
print(x)
cat(y)
}
.,
urls
)}
temp <- webpages_ls %>%
{mapply(
function(x,y) {
print(x)
cat(y)
},
.,
urls
)}
temp <- webpages_ls[1] %>%
{mapply(
function(x,y) {
print(x)
cat(y)
},
.,
urls[1]
)}
temp <- webpages_ls[1] %>%
{mapply(
function(x,y) {
x
y
},
.,
urls[1]
)}
temp
temp <- webpages_ls[1] %>%
{mapply(
function(x,y) {
x[["url"]] <- y
x
},
.,
urls[1]
)}
View(temp)
temp <- webpages_ls[1] %>%
append(urls, after = 1)
View(temp)
temp <- webpages_ls[1] %>%
append(urls, after = 0)
View(temp)
temp <- webpages_ls %>%
{mapply(
function(x, y) {
append(x, y, after = 0)
},
.,
urls
)}
View(temp)
test <- list(element1=list(a=1,b=2,a1=8),
element2=list(a=9,d=17))
test
vec <- c(12,25)
vec
merge <- mapply(c, test, vec)
merge
map(test, vec, c)
purrr:map(test, vec, c)
purrr::map(test, vec, c)
purrr::map2(test, vec, c#function(x, y) {
)
temp <- webpages_ls %>%
purrr::map2(urls, c)
View(temp)
temp <- webpages_ls %>%
temp <- webpages_ls %>%
purrr::map2(
urls,
function(x, y) {
x[["url"]] <- y
x
}
)
View(temp)
View(webpages_ls)
temp <- webpages_ls %>%
purrr::map2(
urls,
c
# function(x, y) {
#     x[["url"]] <- y
#     x
# }
)
View(temp)
temp[["gen_9.core_i7.mem_8.ssd.store_med"]]
## load JTools
library(JTools)
## load JTools
library(JTools)
## load packages
## load packages
install_missing_packages(
c("rvest", "robotstxt", "magrittr")
)
options(stringsAsFactors = FALSE)
rm(base)
rm(base, gen, core, mem, hd, store)
## load JTools
library(JTools)
## load packages
install_missing_packages(
c("rvest", "robotstxt", "magrittr")
)
options(stringsAsFactors = FALSE)
base <- "https://www.dell.com/en-us/shop/dell-laptops/sr/laptops/15-inch?appliedRefinements=15676"
gen <- c(
gen_9 = "18401",
gen_10 = "18402"
)
core <- c(
core_i7 = "6084",
core_i9 = "14158"
)
mem <- c(
mem_8 = "6092",
mem_12 = "6093",
mem_16 = "6094",
mem_24 = "6095",
mem_32up = "6096"#,
# mem_optane = "10157"
)
hd <- c(
ssd = "13241",
hdd = "13242",
hssd = "14105"
)
store <- c(
store_med = "13248",
store_hi = "13249"
)
names <- expand.grid(
names(gen), names(core), names(mem), names(hd), names(store)
) %>%
apply(1, paste, collapse = ".")
urls <- expand.grid(base, gen, core, mem, hd, store) %>%
apply(1, paste, collapse = ",")
rm(base, gen, core, mem, hd, store)
## load JTools
library(JTools)
## load packages
install_missing_packages(
c("rvest", "robotstxt", "magrittr")
)
options(stringsAsFactors = FALSE)
base <- "https://www.dell.com/en-us/shop/dell-laptops/sr/laptops/15-inch?appliedRefinements=15676"
gen <- c(
gen_9 = "18401",
gen_10 = "18402"
)
core <- c(
core_i7 = "6084",
core_i9 = "14158"
)
mem <- c(
mem_8 = "6092",
mem_12 = "6093",
mem_16 = "6094",
mem_24 = "6095",
mem_32up = "6096"#,
# mem_optane = "10157"
)
hd <- c(
ssd = "13241",
hdd = "13242",
hssd = "14105"
)
store <- c(
store_med = "13248",
store_hi = "13249"
)
names <- expand.grid(
names(gen), names(core), names(mem), names(hd), names(store)
) %>%
apply(1, paste, collapse = ".")
urls <- expand.grid(base, gen, core, mem, hd, store) %>%
apply(1, paste, collapse = ",")
rm(base, gen, core, mem, hd, store)
names(webpages_ls) <- names
View(webpages_ls)
webpage <- webpages_ls[[1]]
scrape <- function(element, webpage)  {
# element <- elements[6]
#Using CSS selectors to scrape the rankings section
data_html <- html_nodes(webpage, element)
#Converting the ranking data to text
data_all <- html_text(data_html)
## remove bad data
data <- data_all %>%
{.[(nchar(.) < 100 )]}#& nchar(.) > 0)]}
#Let's have a look at the rankings
as.data.frame(data)
}
temp <- lapply(elements, scrape, webpage)
## load JTools
library(JTools)
## load packages, set options
install_missing_packages(
c("rvest", "robotstxt", "magrittr")
)
options(stringsAsFactors = FALSE)
## identify elements to extract from websites
elements <- c(
count = ".resultsCount",
name = ".fast-delivery-text-link",
price_sale = ".force-strong",
price_regular = ".normal-font-size",
processor = "li:nth-child(1) small",
os = "li:nth-child(2) small",
memory = "li:nth-child(3) small",
hard_drive = "li:nth-child(4) small",
display = ".ups-feature .text-gray-dark"
)
## set url variables
base <- "https://www.dell.com/en-us/shop/dell-laptops/sr/laptops/15-inch?appliedRefinements=15676"
gen <- c(
gen_9 = "18401",
gen_10 = "18402"
)
core <- c(
core_i7 = "6084",
core_i9 = "14158"
)
mem <- c(
mem_8 = "6092",
mem_12 = "6093",
mem_16 = "6094",
mem_24 = "6095",
mem_32up = "6096"#,
# mem_optane = "10157"
)
hd <- c(
ssd = "13241",
hdd = "13242",
hssd = "14105"
)
store <- c(
store_med = "13248",
store_hi = "13249"
)
## generate urls to scrape
urls <- expand.grid(base, gen, core, mem, hd, store) %>%
apply(1, paste, collapse = ",")
## generate "short" names for scraped websites
names <- expand.grid(
names(gen), names(core), names(mem), names(hd), names(store)
) %>%
apply(1, paste, collapse = ".")
## get rid of un-needed variables
rm(base, gen, core, mem, hd, store)
## scrape websites
# webpages_ls <- lapply(urls, read_html)
## name scraped pages
names(webpages_ls) <- names
webpage <- webpages_ls[[1]]
scrape <- function(element, webpage)  {
# element <- elements[6]
#Using CSS selectors to scrape the rankings section
data_html <- html_nodes(webpage, element)
#Converting the ranking data to text
data_all <- html_text(data_html)
## remove bad data
data <- data_all %>%
{.[(nchar(.) < 100 )]}#& nchar(.) > 0)]}
#Let's have a look at the rankings
as.data.frame(data)
}
temp <- lapply(elements, scrape, webpage)
View(temp)
temp[["price_sale"]]
temp[["os"]]
temp[["memory"]]
temp <- webpages_ls %>%
map(
~lapply(elements, scrape, .x)
)
## load packages, set options
install_missing_packages(
c("rvest", "robotstxt", "magrittr", "purrr")
)
## name downloaded pages
names(webpages_ls) <- names
## create scrape function
scrape <- function(element, webpage)  {
## scrape the desired elements
data_html <- html_nodes(webpage, element)
## convert the scraped elements to text
data_all <- html_text(data_html)
## remove bad data
data <- data_all %>%
# {.[(nchar(.) < 100 )]}#& nchar(.) > 0)]}
{.[(nchar(.) < 100 )]} %>%
as.data.frame()
}
temp <- webpages_ls %>%
map(
~lapply(elements, scrape, .x)
)
## load packages, set options
install_missing_packages(
c("rvest", "robotstxt", "magrittr", "purrr")
)
## create scrape function
scrape <- function(element, webpage)  {
## scrape the desired elements
data_html <- html_nodes(webpage, element)
## convert the scraped elements to text
data_all <- html_text(data_html)
## remove bad data
data <- data_all %>%
# {.[(nchar(.) < 100 )]}#& nchar(.) > 0)]}
{.[(nchar(.) < 100 )]} %>%
as.data.frame()
}
temp <- webpages_ls %>%
map(
~lapply(elements, scrape, .x)
)
.packages()
(.packages())
detach("JTools")
detach(JTools)
(.packages())
dir()
getwd()
getwd()
original <- "less /gpfs0/work/joshua.theisen/projects/species_identification/data/HIC11383_S45_L001_R1_001_190927.fastq.gz > \
/gpfs0/work/joshua.theisen/projects/species_identification/test_fastq/L001_R1.fastq
less /gpfs0/work/joshua.theisen/projects/species_identification/data/HIC11383_S45_L001_R2_001_190927.fastq.gz > \
/gpfs0/work/joshua.theisen/projects/species_identification/test_fastq/L001_R2.fastq"
library(magrittr)
for (i in 1:4) {
edited <- original %>%
gsub("(?<L00)1", i, .) %>%
paste0("\n")
cat(edited)
}
for (i in 1:4) {
edited <- original %>%
gsub("(?<=L00)1", i, .) %>%
paste0("\n")
cat(edited)
}
for (i in 1:4) {
edited <- original %>%
(?<=foo)
gsub("(?<=L00)1", i, ., perl = TRUE) %>%
paste0("\n")
cat(edited)
}
for (i in 1:4) {
edited <- original %>%
gsub("(?<=L00)1", i, ., perl = TRUE) %>%
paste0("\n")
cat(edited)
}
script_path <- "B:/shared_with_VM/git/voltron/centrifuge_slurm_scripts"
# script_path <- "B:/shared_with_VM/projects/antibiotic_resistance/centrifuge_pipeline"
script_name <- "centrifuge_base.sh"
output_path <- "generated_scripts"
output_name <- "centrifuge_concatenated.sh"
setwd(script_path)
script <- readChar(
file.path(script_path, script_name),
nchars = 10000000
)
# HIC11489-HIC11516
for (i in 11489:11514) {
j <- as.character(i)
original <- "11383"
new <- gsub(original, j, script)
if (!exists("catted")) {
catted <- new
} else {
catted <- paste0(catted, "\n\n\n\n", new)
}
}
if (!dir.exists(file.path(script_path, output_path))) {
dir.create(file.path(script_path, output_path))
}
writeChar(
catted,
file.path(script_path, output_path, output_name)
)
# for (i in 11489:11516) {
#     j <- as.character(i)
#     original <- "11383"
#     new <- gsub(original, j, base_script)
#     if (!dir.exists(file.path(base_dir, data_dir))) {
#         dir.create(file.path(base_dir, data_dir))
#     }
#     writeChar(
#         new,
#         file.path(base_dir, data_dir, paste0("centrifuge_", j, ".sh"))
#     )
#
# }
